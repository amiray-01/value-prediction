{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prédiction des valeurs d'un joueur de Football"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import des bibliothèques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import make_scorer, mean_absolute_error, root_mean_squared_error, mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players = pd.read_csv(\"data/fifa_players.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysons les valeurs nulles de notre Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(players.isnull().sum()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(players['national_team'].isnull().sum() / len(players) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players = players.drop(columns=['name', 'full_name', 'birth_date', 'national_team_position', 'national_jersey_number','national_team', 'national_rating' ])\n",
    "players.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On supprime les colonnes clause libératoire et salaire car elles risquent de donner trop d'indices à notre modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players = players.drop(columns=['release_clause_euro', 'wage_euro'])\n",
    "players.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(players.isnull().sum()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " On se débarasse des valeurs nulles de la valeur cible "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players = players.dropna(subset=['value_euro'])\n",
    "print(players.isnull().sum()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nettoyage et traitement des autres colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(players['positions'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = players['positions'].str.get_dummies(sep=',')\n",
    "print(positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players = pd.concat([players, positions], axis=1)\n",
    "players.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players.drop(columns=['positions'], inplace=True)\n",
    "players.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici on a une fonction qui traduit les True/False obtenu en 1/0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_columns_to_int(df, columns):\n",
    "    for column in columns:\n",
    "        df[column] = df[column].astype('int')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(players['nationality'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici nous avons le TOP 10 des nations de l'année 2018 et le traitement de la colonne nationalité"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_nations = ['Belgium', 'France', 'Brazil', 'Croatia', 'England', 'Portugal', 'Uruguay', 'Switzerland', 'Spain', 'Denmark', 'Argentina']\n",
    "players['top_nation'] = players['nationality'].apply(lambda x: 1 if x in top_nations else 0)\n",
    "players.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(players['preferred_foot'].unique())\n",
    "print(players['preferred_foot'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players = pd.get_dummies(players, columns = ['preferred_foot'], prefix = 'foot', drop_first = False)\n",
    "convert_columns_to_int(players, ['foot_Left', 'foot_Right'])\n",
    "players.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(players['body_type'].unique())\n",
    "print(players['body_type'].value_counts()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players['body_type'] = players['body_type'].replace(['Messi','C. Ronaldo','Neymar', 'Courtois', 'Akinfenwa', 'Shaqiri', 'PLAYER_BODY_TYPE_25'], 'Normal')\n",
    "print(players['body_type'].unique())\n",
    "print(players['body_type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players = pd.get_dummies(players, columns = ['body_type'], prefix = 'body', drop_first = False)\n",
    "players.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_columns_to_int(players, ['body_Lean', 'body_Normal', 'body_Stocky'])\n",
    "players.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players = players.drop(columns=['nationality'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(players[['body_Lean', 'body_Normal', 'body_Stocky']].sum(axis=1).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On affiche juste les minimums et maximums de chaque colonne "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in players.columns:\n",
    "    print(f\"{col}: {players[col].min(), players[col].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On passe la valeur de joueur en `Log` base 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players['log_value_euro'] = np.log10(players['value_euro'])\n",
    "players.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici on a une fonction qui permet de visualiser la distribution d'une colonne "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribution(data, column):\n",
    "    sns.histplot(data[column], bins=50, kde=True)\n",
    "    plt.title(\"Distribution de \" + column)\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel(\"Nombre de joueurs\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution(players, 'value_euro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution(players, 'log_value_euro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici on a une fonction qui permet de mesurer la corrélation entre les variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_scatterplot(data, x, y):\n",
    "    sns.scatterplot(data=data, x=x, y=y)\n",
    "    plt.title(\"Relation entre \"+x+\" et \"+y)\n",
    "    plt.xlabel(x)\n",
    "    plt.ylabel(y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "players['potential_to_rating'] = players['potential'] / players['overall_rating']\n",
    "players['bmi'] = players['weight_kgs'] / ((players['height_cm'] / 100) ** 2)\n",
    "players['weighted_reputation'] = players['overall_rating'] * players['international_reputation(1-5)']\n",
    "players['age_to_potential'] = (players['potential'] - players['overall_rating']) / players['age']\n",
    "players['decline_risk'] = players['age'] / players['potential'] \n",
    "players['pace'] = players[['acceleration', 'sprint_speed']].mean(axis=1)\n",
    "players['shoot'] = players[['finishing', 'penalties', 'volleys','shot_power', 'long_shots','positioning']].mean(axis=1)\n",
    "players['pass'] = players[['vision', 'crossing', 'freekick_accuracy', 'short_passing', 'long_passing', 'curve']].mean(axis=1)\n",
    "players['dribble'] = players[['agility', 'balance','reactions','ball_control', 'dribbling', 'composure']].mean(axis=1)\n",
    "players['defend'] = players[['interceptions', 'heading_accuracy','standing_tackle','sliding_tackle', 'marking']].mean(axis=1)\n",
    "players['physical'] = players[['jumping','stamina', 'strength', 'aggression']].mean(axis=1)\n",
    "\n",
    "positions_cols = ['CAM','CB', 'CDM', 'CF', 'CM', 'GK', 'LB', 'LM', 'LW', 'LWB', 'RB', 'RM', 'RW', 'RWB', 'ST']\n",
    "\n",
    "players['versatile'] = players[positions_cols].sum(axis=1)\n",
    "players['versatile_x_rating'] = players['versatile'] * players['overall_rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove = ['crossing', 'finishing',\n",
    "       'heading_accuracy', 'short_passing', 'volleys', 'dribbling', 'curve',\n",
    "       'freekick_accuracy', 'long_passing', 'ball_control', 'acceleration',\n",
    "       'sprint_speed', 'agility', 'reactions', 'balance', 'shot_power',\n",
    "       'jumping', 'stamina', 'strength', 'long_shots', 'aggression',\n",
    "       'interceptions', 'positioning', 'vision', 'penalties', 'composure',\n",
    "       'marking', 'standing_tackle', 'sliding_tackle']\n",
    "\n",
    "players.drop(columns = columns_to_remove, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici on s'est rendu compte que les gardiens allaient être problématique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(players['GK'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On les supprime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players = players[players['GK'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution(players, 'height_cm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(players['height_cm'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suspect_sizes = [152.40, 154.94] \n",
    "players.loc[players['height_cm'].isin(suspect_sizes), 'height_cm'] = np.nan\n",
    "players['known_height'] = players['height_cm'].notna().astype(int)\n",
    "\n",
    "print(players['known_height'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On filtre les lignes sans NaN dans les deux colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = players[['height_cm', 'value_euro', 'log_value_euro']].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici on vérifie les corrélations avec pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_value = valid_data['height_cm'].corr(valid_data['value_euro'])\n",
    "corr_log_value = valid_data['height_cm'].corr(valid_data['log_value_euro'])\n",
    "\n",
    "print(f\"Corrélation entre height_cm et value_euro : {corr_value:.4f}\")\n",
    "print(f\"Corrélation entre height_cm et log_value_euro : {corr_log_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici nous avons une fonction qui permet de calculer la médiane pondérée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_height(row):\n",
    "    if pd.isna(row['height_cm']):\n",
    "        positions = row[positions_cols]\n",
    "        player_positions = positions[positions == 1].index.tolist()\n",
    "        \n",
    "        if len(player_positions) > 0:\n",
    "            median_height = players[players[player_positions].sum(axis=1) > 0]['height_cm'].median()\n",
    "            return median_height\n",
    "    return row['height_cm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players['height_cm'] = players.apply(impute_height, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Devrait affiché `0` car nous venons de la traiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(players['height_cm'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(players[['height_cm', 'known_height']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training / Validation / Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Séparation du dataset en deux ensembles: Train / Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = players.drop(columns=['value_euro', 'log_value_euro'])\n",
    "Y = players['value_euro']\n",
    "Y_log = players['log_value_euro']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y_log, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici nous avons défini des fonctions qui permettent de récuperer le resultat de ces métriques avec les valeurs réélles et non en `log scale` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_original(Y_true, Y_pred_log):\n",
    "    Y_true_orig = 10**Y_true\n",
    "    Y_pred_orig = 10**Y_pred_log\n",
    "    return root_mean_squared_error(Y_true_orig, Y_pred_orig)\n",
    "\n",
    "def mae_original(Y_true, Y_pred_log):\n",
    "    Y_true_orig = 10**Y_true\n",
    "    Y_pred_orig = 10**Y_pred_log\n",
    "    return mean_absolute_error(Y_true_orig, Y_pred_orig)\n",
    "\n",
    "def mape_original(Y_true, Y_pred_log):\n",
    "    Y_true_orig = 10**Y_true\n",
    "    Y_pred_orig = 10**Y_pred_log\n",
    "    return mean_absolute_percentage_error(Y_true_orig, Y_pred_orig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Petit `heatmap` pour voir de près la corrélation entre les différents colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(X_train.corr(), cmap=\"coolwarm\", annot=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisation de la corrélation entre les colonnes que nous jugeons suscpetibles d'être les plus impactantes pour le training du model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=players, x_vars=['overall_rating','potential', 'decline_risk', 'weighted_reputation'], y_vars='log_value_euro', kind='scatter')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 models séléctionnés: `Linear Regression` et `Random Forest` \n",
    "#### Validation croisée en utilisant différentes métriques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"RandomForest\": RandomForestRegressor(random_state=42)\n",
    "}\n",
    "scoring = {\n",
    "    'MAE': make_scorer(mean_absolute_error, greater_is_better=False),\n",
    "    'RMSE': make_scorer(root_mean_squared_error, greater_is_better=False),\n",
    "    'MAPE': make_scorer(mean_absolute_percentage_error, greater_is_better=False),\n",
    "    'RMSE_original': make_scorer(rmse_original, greater_is_better=False),\n",
    "    'MAE_original': make_scorer(mae_original, greater_is_better=False),\n",
    "    'MAPE_original': make_scorer(mape_original, greater_is_better=False)\n",
    "}\n",
    "results = {}\n",
    "for model_name, model in models.items():\n",
    "    cv_results = cross_validate(\n",
    "        model, \n",
    "        X_train, \n",
    "        Y_train, \n",
    "        cv=5, \n",
    "        scoring=scoring, \n",
    "        return_train_score=True\n",
    "    )\n",
    "    results[model_name] = {\n",
    "        \"RMSE\": cv_results['test_RMSE'],\n",
    "        \"RMSE_mean\": np.mean(cv_results['test_RMSE']),\n",
    "        \"MAE\": cv_results['test_MAE'],\n",
    "        \"MAE_mean\": np.mean(cv_results['test_MAE']),\n",
    "        \"MAPE\": cv_results['test_MAPE'],\n",
    "        \"MAPE_mean\": np.mean(cv_results['test_MAPE']),\n",
    "        \"RMSE_original\": cv_results['test_RMSE_original'],\n",
    "        \"RMSE_original_mean\": np.mean(cv_results['test_RMSE_original']),\n",
    "        \"MAE_original\": cv_results['test_MAE_original'],\n",
    "        \"MAE_original_mean\": np.mean(cv_results['test_MAE_original']),\n",
    "        \"MAPE_original\": cv_results['test_MAPE_original'],\n",
    "        \"MAPE_original_mean\": np.mean(cv_results['test_MAPE_original'])\n",
    "    }\n",
    "print(\"\\n--- Résumé des résultats ---\")\n",
    "for model_name, metrics in results.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    for metric, values in metrics.items():\n",
    "        if \"mean\" in metric:\n",
    "            print(f\"  {metric}: {-values:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tunning du `Random Forest` afin d'optimiser les performances (Cela peut être long ... très long en fonction de la machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 500],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 3, 5, 10],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'bootstrap': [True]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=RandomForestRegressor(random_state=42),\n",
    "    param_grid=param_grid,\n",
    "    scoring={\n",
    "        'MAPE': 'neg_mean_absolute_percentage_error',\n",
    "        'RMSE': 'neg_root_mean_squared_error',\n",
    "    },\n",
    "    refit='MAPE',\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Best parameters: \", grid_search.best_params_)\n",
    "print(\"Best value for MAPE: \", -grid_search.best_score_)\n",
    "\n",
    "cv_results = grid_search.cv_results_\n",
    "rmse_scores = cv_results['mean_test_RMSE']\n",
    "\n",
    "best_rmse_idx = rmse_scores.argmax()\n",
    "best_rmse = -rmse_scores[best_rmse_idx]\n",
    "best_rmse_params = cv_results['params'][best_rmse_idx]\n",
    "print(f\"\\nMeilleure valeur de RMSE : {best_rmse:.4f} pour les paramètres : {best_rmse_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meilleurs paramètres pour MAPE : {'n_estimators': 500, 'bootstrap': True, 'max_depth': 20, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 2} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meilleurs paramètres pour RMSE : {'bootstrap': True, 'max_depth': 30, 'max_features': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation du model sur l'ensemble de `Test` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(n_estimators= 500, bootstrap=  True, max_depth=20, max_features=None, min_samples_leaf=1, min_samples_split=2)\n",
    "model.fit(X_train, Y_train)\n",
    "Y_pred = model.predict(X_test)\n",
    "\n",
    "rmse_test_log = root_mean_squared_error(Y_test,Y_pred)\n",
    "mae_test_log = mean_absolute_error(Y_test, Y_pred)\n",
    "mape_test = mean_absolute_percentage_error(Y_test, Y_pred)\n",
    "\n",
    "Y_test_orig = 10 ** Y_test\n",
    "Y_pred_orig = 10 ** Y_pred\n",
    "\n",
    "rmse_test_orig = root_mean_squared_error(Y_test_orig, Y_pred_orig)\n",
    "mae_test_orig = mean_absolute_error(Y_test_orig, Y_pred_orig)\n",
    "mape_test_orig = mean_absolute_percentage_error(Y_test_orig, Y_pred_orig)\n",
    "\n",
    "print(f\"RMSE (log scale): {rmse_test_log:.4f}\")\n",
    "print(f\"MAE (log scale): {mae_test_log:.4f}\")\n",
    "print(f\"MAPE (log scale): {mape_test:.4f}\")\n",
    "print(f\"RMSE (original scale): {rmse_test_orig:.4f}\")\n",
    "print(f\"MAE (original scale): {mae_test_orig:.4f}\")\n",
    "print(f\"MAPE (original scale): {mape_test_orig:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Petit figure pour visualiser résumer les resultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(Y_test_orig, Y_pred_orig, alpha=0.7)\n",
    "plt.plot([Y_test_orig.min(), Y_test_orig.max()], [Y_test_orig.min(), Y_test_orig.max()], 'r--')\n",
    "plt.xlabel(\"Valeurs réelles (en euros)\")\n",
    "plt.ylabel(\"Valeurs prédites (en euros)\")\n",
    "plt.title(\"Comparaison des prédictions\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
